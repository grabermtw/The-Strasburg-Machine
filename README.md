# The Strasburg Machine
### By [Matthew Graber](https://github.com/grabermtw), [Matthew Vorsteg](https://github.com/mvorsteg), [Jordan Woo](https://github.com/minersail), [Timothy Henderson](https://github.com/timhenderson17), and [Zhiyuan (Ben) Xue](https://github.com/zxue02)

## Abstract
In this project, we experiment with training an agent to throw a ball in a 3D environment. We use evolutionary strategies to pass down traits, such as torque applied to specific joints of an arm and release time, between generations. Our project aims to create an evolutionary strategy such that agents learn to throw a ball. We are chiefly concerned with the effect of allowing the fittest agents from a current generation into the population for a subsequent generation. In our experiment, we measure the maximum and average fitness of a generation while varying the percentages of fit individuals that persist across generations unchanged. Our results show that our approach is effective at forcing the agent to learn quickly, and agents learn faster and show the most improvement when 30% of the fittest individuals survive to the next generation. Our results lay the framework for genetically engineered agents in video games where complex locomotions are required. 
## Introduction
In this project, we set out to investigate the possibility of training human-like, 3D characters in the Unity game engine to throw a ball. It is common to use machine learning methods to teach agents how to move and behave in both 2D and 3D game engines. In the past, we have seen demonstrations of evolution strategies and genetic algorithms being used for this purpose, training agents to navigate obstacle courses and other environments. For this project, we sought to train the agents to perform a realistic task (in this case, throwing a ball) as efficiently as possible, laying the foundations for the eventual creation of realistic auto-generated animations for 3D characters in video games where complex and unusual behaviors are necessary.

Past and current uses of evolutionary computation in this area has focused on using genetic algorithms for locomotion, in particular bipedal and quadrupedal locomotion of robots. Locomotion is commonly studied using genetic algorithms because it originally came about from natural selection but, perhaps more importantly, also because of the relatively straightforward fitness function. A recent research by Jakub Hulas and Chengxu Zhou (2020) used a genetic algorithm to improve the gait of a quadruped robot on sand. Walking on sand is difficult for robots because the feet penetrate the sand while walking. A 2D gait was given to the algorithm initially and the algorithm generated 3D gaits. The algorithm ultimately generated an effective gait, optimising the joint space trajectory. This research demonstrates the power of genetic algorithms even in special circumstances, in this case locomotion on sand. 

While locomotion was the focus, there have been studies and research on effectively moving arms using genetic algorithms. This task is considered more difficult because an arm can perform more functions, has more freedom, and requires more precision than a leg. Certain arm functions, such as pick-and-place, even require extreme precision. Ligutan et al. (2019) used genetic algorithms to carry out motion planning for a robotic arm with 4-DOF. It did so by changing a variable-length genome of joint angles. The developed genetic algorithm, with an adaptive linear interpolation crossover to improve convergence, effectively generated paths to its target while being able to avoid obstacles. However, on average the error was 1.4 mm. So, even though the arm was not complicated (only 4-DOF), it still cannot handle tasks that would require extreme precision.

Previous works have highlighted the effectiveness of genetic algorithms in generating solutions in complicated scenarios, particularly relating to locomotion. However, genetic algorithms still fall short when applied to other body parts, particularly the arm. In this research study, we seek to explore this weakness. Throwing a ball normally does not require extreme precision but throwing a ball as far as possible does (e.g. when to release and the torques of the joints). Thus, in this way, we can explore how far evolutionary computation can take us in respect to the precision of an arm.

In addition to simply training the agents to throw the ball, we sought to determine the best method for using an evolution strategy to train the agents for this task (i.e. number of agents, percentage of parents to keep, crossover probability, mutation probability, etc.). We hypothesized that for each training session, a sharp increase in fitness (the distance the ball is thrown in the direction the agent is facing) would be observed initially, followed by an asymptotic leveling as the generations increase. In order to achieve this, we set out to accomplish the following objectives:

1. Setting up the environment and the 3D agents within the Unity game engine.
2. Designing and implementing an evolutionary learning method that gives usable parameters to the agents.
3. Accounting for nondeterministic physics within the Unity game engine.
4. Implementing a way to export the data from the program in a usable format for further analysis.
5. Building out the project so that the simulation can easily be run with different parameters to obtain data.

In the rest of this report, we will discuss the methods we followed to obtain these above objectives, including the different parameters involved in the evolutionary learning algorithm chosen. Following this, we will discuss the methods that we used to collect data from our algorithm and the results obtained from testing. Lastly, we will analyze key results and discuss the significance of our findings with respect to our original goals, objectives, and hypothesis.
## Methods
In order to approach this project, we started by creating a template for an agent in a simple 3D environment. This agent has moveable joints in its right shoulder, arm, elbow, and wrist. The agent is also holding a ball that it will release from its grip after a set amount of time. Unity provides the environment with a realistic physics engine, so that gravity is applied to the ball and the agent’s arm. Thus, the agent is capable of throwing the ball, but does not know how to move its arm to accomplish this. Our goal with this is to implement an evolutionary learning method in which agents learn, over numerous generations, how to throw a ball.

In deciding which evolutionary learning technique was best, we looked at both standard genetic algorithms and evolution strategies. Because of the nature of the task we set out to accomplish and the environment (Unity) that we would be using to implement our evolutionary computation, we decided it was best to use an evolution strategy. Our agents’ chromosomes are real-valued vectors containing the information needed to perform a single throw. For the four joints, we control the torque applied in the x, y, and z direction to the joint at the start of the throw. For the last value in the vector, we control the release frame of the ball. More colloquially, this value will control how long the agent holds onto the ball before releasing it. Given these parameter values, an agent will be able to make an attempt to throw the ball. Keeping these values in a vector is better suited for this application, as there is no easy way to convert all values in the vector into a binary string and still easily be able to recognize the genes present in each chromosome. Keeping the values separate in a vector is more readable and easier to distinguish between genes.

We implemented our evolutionary strategy as a C# script in Unity to oversee the progression of our population. We first define our fitness function as the distance the ball traveled in the z-direction before hitting the ground. Given that we wanted the agents to throw as far as possible, keeping track of the total distance a ball travels didn’t make sense, so individuals were allowed to learn many different throwing techniques. Keeping track of the distance in one direction also prevented individuals that throw sideways from being fitter than those throwing straight. In determining an agent’s fitness, we also make them throw a certain number of throws. This is necessary to account for “flukes” in the data when an agent makes an inconsistent throw. Unity’s physics engine is non-deterministic, so seemingly unrealistic data may be generated from glitches. The median value of the agent’s throws is taken as its final fitness score, which makes it resistant to outliers, both high and low.

When choosing individuals to crossover for the next generation, we used a fitness-proportionate selection so that fitter individuals have the best chance to pass on their traits and weaker individuals will have a small chance. We select our individuals using a roulette wheel style selection where the chance of being selected is directly proportional to an individual's fitness. After choosing the individuals, we determine if crossover using a set crossover probability, pc. We decided between three options for the method by which we perform crossover. The first option was to use single point crossover. Doing this would mean simply choosing a position in the two individual’s vector’s and crossing over beyond that part. We decided that there was no logical way to determine where that crossover point should be. Another option we had to consider was whole-arithmetic recombination. This method treats the two chosen individuals as parents, and creates a child whose weights are the average of the parents’ values. We believed that this method would make the children less fit because the torque values and release point are quite sensitive. The crossover method we chose was a uniform crossover. With this method, each joint of a child created by the parents had an equal chance of being the torque value from parent one and parent two. This follows the Building Block Hypothesis, as the torques applied to each joint release point create a unique chromosome for an individual agent, and each variable (building block) can be combined during crossover and reproduction to create more fit individuals.

After crossover, there is a small probability, pm, that an individual will be mutated before it moves on to the next generation. Our mutation is performed by generating a new, random value for the x, y, and z torque values for a particular joint. With a small chance of mutation, our population should maintain genetic diversity from one generation to the next.

This algorithm successfully allows for agents to learn to throw, but we can also experiment with the fastest ways our agent can learn, as there are a number of hyperparameters that we can manually adjust before learning. We can alter pc and pm as well as the number of agents in each generation and the percent of parents carried over from one generation to the next. These standard parameters are modified before learning. 

Lastly, in order to make sure that fitter individuals would have an impact on the next generation, we allow for a certain percentage of the agents in a generation to automatically be used in the next generation. Including a small percentage of the fittest individuals is a common technique in many genetic and evolutionary algorithms. In our experiment, we measured the maximum and average fitness per generation while modifying the percentage of fittest individuals allowed into the next generation and keeping all other parameters fixed. Due to the many different parameters for the model itself, we decided to look into the specific effect of changing this one parameter and seeing how effective the evolutionary strategy works.

Trials were run using the algorithm mentioned above in Unity. Each trial consisted of measuring and recording the max fitness and average fitness of each generation for 100 generations. Percentage of fittest individuals kept and number of agents in each generation varied between trials.

## Results
